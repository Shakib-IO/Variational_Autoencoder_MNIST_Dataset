{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ75J0AvsUhH"
      },
      "source": [
        "\r\n",
        "Before know about *Variational Autoencoder*, First you need to understand What is an AutoEncoder.\r\n",
        "# AutoEncoder\r\n",
        "\r\n",
        "To explain simlply, **the autoencoder is a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the input.**\r\n",
        "\r\n",
        "Another Define would be An Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of **representation learning.** Specifically, we'll design a neural network architecture such that we impose a **bottleneck** in the network which forces a compressed knowledge representation of the original input. Let's see the Architecture \r\n",
        "\r\n",
        "<image src =  \"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-06-at-3.17.13-PM.png\">\r\n",
        "\r\n",
        "As visualized above, we can take an unlabeled dataset and feed into neural network and the input x and frame it as a supervised learning problem tasked with outputting x̂.The autoencoder tried to reconstruction of the original input x.\r\n",
        "\r\n",
        "This network can be trained by minimizing the reconstruction error, L(x,x̂), which measures the differences between our original input and the consequent reconstruction.\r\n",
        "\r\n",
        "When we tring to minimizing the error sometimes our network tend to memorize some information or they showing some overfitting results. To avoid this circumstances we need to add L1 regularization with our loss fucntion.\r\n",
        "\r\n",
        "- L(x,x̂) + regularization\r\n",
        "\r\n",
        "Regularization = ${\\cal L}\\left( {x,\\hat x} \\right) +  \\lambda \\sum\\limits_i {\\left| {a_i^{\\left( h \\right)}} \\right|}$\r\n",
        "\r\n",
        "I'm not gonna dive in the details of the math intuition. A great explanation by Andrew Ng [Video](https://youtu.be/u73PU6Qwl1I)\r\n",
        "\r\n",
        "[AutoEncoder explain by Jeremy jordan](https://www.jeremyjordan.me/autoencoders/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iaxz5CUmy7y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}