{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ75J0AvsUhH"
      },
      "source": [
        "\r\n",
        "Before know about *Variational Autoencoder*, First you need to understand What is an AutoEncoder.\r\n",
        "# AutoEncoder\r\n",
        "\r\n",
        "To explain simlply, **the autoencoder is a neural network that learns to copy its input to its output. It has an internal (hidden) layer or latent space that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the input.**\r\n",
        "\r\n",
        "Another Define would be An Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of **representation learning.** Specifically, we'll design a neural network architecture such that we impose a **bottleneck** in the network which forces a compressed knowledge representation of the original input. Let's see the Architecture \r\n",
        "\r\n",
        "<image src =  \"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-06-at-3.17.13-PM.png\">\r\n",
        "\r\n",
        "As visualized above, we can take an unlabeled dataset and feed into neural network and the input x and frame it as a supervised learning problem tasked with outputting x̂.The autoencoder tried to reconstruction of the original input x.\r\n",
        "\r\n",
        "This network can be trained by minimizing the reconstruction error, L(x,x̂), which measures the differences between our original input and the consequent reconstruction.\r\n",
        "The Loss function equation :\r\n",
        "\r\n",
        "<img src = \"https://wikimedia.org/api/rest_v1/media/math/render/svg/2c9d074db8e3d0e19f9f5128edcc26a2e7baad36\">\r\n",
        "\r\n",
        "\r\n",
        "$ where-{\\displaystyle \\mathbf {\\sigma '} ,\\mathbf {W'} ,{\\text{ and }}\\mathbf {b'} } $\r\n",
        "for the **decoder** may be unrelated to the corresponding ${\\displaystyle \\mathbf {\\sigma } ,\\mathbf {W} ,{\\text{ and }}\\mathbf {b} }$ for the **encoder**.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "When we tring to minimizing the error sometimes our network tend to memorize some information or they showing some overfitting results. To avoid this circumstances we need to add L1 regularization with our loss fucntion.\r\n",
        "\r\n",
        "- L(x,x̂) + regularization\r\n",
        "\r\n",
        "Regularization = ${\\cal L}\\left( {x,\\hat x} \\right) +  \\lambda \\sum\\limits_i {\\left| {a_i^{\\left( h \\right)}} \\right|}$\r\n",
        "\r\n",
        "I'm not gonna dive in the details of the math intuition. A great explanation by Andrew Ng [Video](https://youtu.be/u73PU6Qwl1I)\r\n",
        "\r\n",
        "[AutoEncoder explain by Jeremy jordan](https://www.jeremyjordan.me/autoencoders/)\r\n",
        "\r\n",
        "[Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChJLALtvi8XB"
      },
      "source": [
        "Okay !! Now let's talk about the Variational AutoEncoder\r\n",
        "\r\n",
        "# Variational Autoencoder\r\n",
        "\r\n",
        "Variational autoencoder (VAE), one of the approaches to unsupervised learning of complicated distributions. A VAE could do compressing data, reconstructing noisy or corrupted data, interpolating between real data, and are capable of sourcing new concepts and connections from copious amounts of unlabelled data. VAEs are built on top of neural networks.\r\n",
        "\r\n",
        "**A variational autoencoder (VAE) provides a probabilistic manner for describing an observation in latent space. Thus, rather than building an encoder which outputs a single value to describe each latent state attribute, we'll formulate our encoder to describe a probability distribution for each latent attribute.**\r\n",
        "\r\n",
        "So, What the heck mean by probability distribution in the latent space ?\r\n",
        "- **Autoencoder** : What we did in autoencoder is we take input and encoding them and store the information in 3D or 5D latent space. And from the bottleneck, we start reconstructing the output with minimum loss function. \r\n",
        "- **VAE** :  We take unlabel input images and apply encoder, then in the latent space we divided into two distribution which are (mean & standard deviation) and other one is sample distribution. We take sample distribution from the mean & standard deviation and from that distribution tried to reconstruct the output with minimum loss. The pictorial view of VAE\r\n",
        "<img src = \"https://raw.githubusercontent.com/skeydan/whyR2019/master/vae.png\">\r\n",
        "\r\n",
        "Let's see an example of the distribution:\r\n",
        "<img src = \"https://www.jeremyjordan.me/content/images/2018/06/Screen-Shot-2018-06-20-at-2.48.42-PM.png\">\r\n",
        "\r\n",
        "Okay now see how the loss function minimize. In the variational autoencoder the loss function is little complex to understand.\r\n",
        "\r\n",
        "One of the approch is the Varitational inference. For this we need to minimize the KL divergence and maximize the marginal likelihood\r\n",
        "\r\n",
        "\r\n",
        "> Minimize the Kullback-Leibler divergence between two probability distributions q(z) and p(z|y) is defined as:<br>\r\n",
        "$ \\text{KL}(q(z) | p(z|y)) = \\mathbb{E}_{q(z)}\\left[\\ln\\frac{q(z)}{p(z|y)}\\right] $\r\n",
        "\r\n",
        "we can minimize the above expression by maximizing the following\r\n",
        "\r\n",
        "> ${E_{q\\left( {z|x} \\right)}}\\log p\\left( {x|z} \\right) - KL\\left( {q\\left( {z|x} \\right)||p\\left( z \\right)} \\right)$\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "[Jeremy Jordan](https://www.jeremyjordan.me/variational-autoencoders/)<br>\r\n",
        "**[Math Expation by Ali Ghodsi](https://youtu.be/uaaqyVS9-rM?list=LL)**<br>\r\n",
        "[Math Details](https://davidstutz.de/the-mathematics-of-variational-auto-encoders/)<br>\r\n",
        "[Understanding Variational Autoencoders (VAEs)](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iaxz5CUmy7y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}